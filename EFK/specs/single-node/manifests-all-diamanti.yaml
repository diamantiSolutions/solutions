---
# Create Namespace for all of this
---
apiVersion: v1
kind: Namespace
metadata:
  name: logging
  labels:
    name: logging
    stack: logging

# Get the RBAC needs taken care of
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: elasticsearch
  namespace: logging
  labels:
    app: elasticsearch
    stack: logging
rules:
- apiGroups:
  - extensions
  resources:
  - podsecuritypolicies
  resourceNames:
  - elasticsearch
  verbs:
  - use
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: elasticsearch
  namespace: logging
  labels:
    app: elasticsearch
    stack: logging
subjects:
- kind: ServiceAccount
  name: elasticsearch
  namespace: logging
roleRef:
  kind: ClusterRole
  name: elasticsearch
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: elasticsearch
  namespace: logging
  labels:
    app: elasticsearch
    stack: logging
---
apiVersion: extensions/v1beta1
kind: PodSecurityPolicy
metadata:
  name: elasticsearch
  namespace: logging
  labels:
    app: elasticsearch
    stack: logging
spec:
  fsGroup:
    rule: RunAsAny
  privileged: true
  runAsUser:
    rule: RunAsAny
  seLinux:
    rule: RunAsAny
  allowedCapabilities:
  - 'IPC_LOCK'
  - 'SYS_RESOURCE'
  supplementalGroups:
    rule: RunAsAny
  volumes:
  - '*'
  hostPID: true
  hostIPC: true
  hostNetwork: true
  hostPorts:
  - min: 1
    max: 65536

# Setup the Elastic ConfigMap
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: elasticsearch
  namespace: logging
  labels:
    app: elasticsearch
    stack: logging
data:
  elasticsearch.yml: |
    cluster.name: full-stack-cluster
    node.name: node-1
    path.data: /usr/share/elasticsearch/data
    http:
      host: 0.0.0.0
      port: 9200
    bootstrap.memory_lock: true
    transport.host: 127.0.0.1

# Setup the Elastic Deployment
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: elasticsearch
  namespace: logging
  labels:
    app: elasticsearch
    stack: logging
spec:
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: elasticsearch
        stack: logging
    spec:
      serviceAccountName: elasticsearch
      # This is needed in order to up the mmapfs limits since elastic uses it
      # https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html
      # We do not have to do this if the underlying system is already configured and can drop the privileged access
      initContainers:
      - name: set-vm-max-map-count
        image: busybox
        imagePullPolicy: IfNotPresent
        command: ['sysctl', '-w', 'vm.max_map_count=262144']
        securityContext:
          privileged: true
      - name: volume-mount-hack
        image: busybox
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c", "chown -R 1000:100 /usr/share/elasticsearch/data"]
        volumeMounts:
        - name: elastic-data
          mountPath: /usr/share/elasticsearch/data
      containers:
      # https://www.elastic.co/guide/en/elasticsearch/reference/6.2/docker.html
      - name: elasticsearch
        # TODO: need to find a good way to keep the version updated on this as elastic doesnt provide a stable or "latest" tag
        image: docker.elastic.co/elasticsearch/elasticsearch-oss:6.2.3
        imagePullPolicy: IfNotPresent
        env:
        - name: ES_JAVA_OPTS
          value: -Xms2048m -Xmx2048m
        ports:
        - containerPort: 9200
        resources:
          limits:
            memory: "4Gi"
        volumeMounts:
        - name: config
          mountPath: /usr/share/elasticsearch/elasticsearch.yml
          subPath: elasticsearch.yml
        - name: elastic-data
          mountPath: /usr/share/elasticsearch/data
      # Allow non-root user to access PersistentVolume
      # https://www.elastic.co/guide/en/elasticsearch/reference/6.2/docker.html
      securityContext:
        fsGroup: 1000
      restartPolicy: Always
      volumes:
      - name: config
        configMap:
          name: elasticsearch
      - name: elastic-data
        persistentVolumeClaim:
          claimName: elastic-data-1

# Setup the Elastic Ingress point
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: elasticsearch
  namespace: logging
  labels:
    app: elasticsearch
    stack: logging
spec:
  rules:
  - host: elasticsearch.logging.svc.solutions.eng.diamanti.com
    http:
      paths:
      - path: /
        backend:
          serviceName: elasticsearch
          servicePort: 9200

# The PV is not needed in order to use Diamanti storage controllers
# Instead we will just let the dynamic provisioner setup the volume for us
# by using a PVC only
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: elastic-data-1
  namespace: logging
  labels:
    app: elasticsearch
    stack: logging
spec:
  storageClassName: high
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi

# Setup the Elastic Service and Endpoint
---
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch
  namespace: logging
  labels:
    app: elasticsearch
    stack: logging
spec:
  clusterIP: None
  type: ClusterIP
  ports:
  - name: "api"
    port: 9200
    targetPort: 9200
  selector:
    app: elasticsearch
    stack: logging

# Setup the FluentD RBAC
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: fluentd
  labels:
    app: fluentd
    stack: logging
rules:
- apiGroups:
  - extensions
  resources:
  - podsecuritypolicies
  resourceNames:
  - fluentd
  verbs:
  - use
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: fluentd
  namespace: logging
  labels:
    app: fluentd
    stack: logging
subjects:
- kind: ServiceAccount
  name: fluentd
roleRef:
  kind: Role
  name: fluentd
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluentd
  namespace: logging
  labels:
    app: fluentd
    stack: logging
---
apiVersion: extensions/v1beta1
kind: PodSecurityPolicy
metadata:
  name: fluentd
  namespace: logging
  labels:
    app: fluentd
    stack: logging
spec:
  privileged: true
  allowPrivilegeEscalation: true
  fsGroup:
    rule: RunAsAny
  runAsUser:
    rule: RunAsAny
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  volumes:
  - emptyDir
  - secret
  - downwardAPI
  - configMap
  - persistentVolumeClaim
  - projected
  - hostPath
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluentd
  namespace: logging
  labels:
    app: fluentd
    stack: logging
rules:
- apiGroups: [""] # core API group
  resources: ["pods", "namespaces"]
  verbs: ["get", "watch", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fluentd
  namespace: logging
  labels:
    app: fluentd
    stack: logging
subjects:
- kind: ServiceAccount
  name: fluentd
  namespace: logging
roleRef:
  kind: ClusterRole
  name: fluentd
  apiGroup: rbac.authorization.k8s.io

# Setup the FluentD ConfigMap
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd
  namespace: logging
  labels:
    app: fluentd
data:
  # see also https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-elasticsearch/fluentd-es-image/td-agent.conf
  # https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter
  # https://github.com/fabric8io/docker-fluentd-kubernetes/issues/11 - In order to try and workaround this issue we use the plugins below
  # fluent-plugin-concat, fluent-plugin-parser, rewrite_tag_filter, fluent-plugin-grok-parser

  # Setup the default connection from FLuentD to Elastic
  # as well as add some sanity numbers for options to try and make sure we can keep up with log input
  fluent.conf: |
    @include kubernetes.conf

    <match **>
      type elasticsearch
      log_level info
      include_tag_key true
      host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
      port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
      scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME'] || 'http'}"
      # user "#{ENV['FLUENT_ELASTICSEARCH_USER']}"
      # password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD']}"
      reload_connections "#{ENV['FLUENT_ELASTICSEARCH_RELOAD_CONNECTIONS'] || 'true'}"
      logstash_prefix "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX'] || 'logstash'}"
      logstash_format true
      buffer_chunk_limit 2M
      buffer_queue_limit 32
      flush_interval 5s
      max_retry_wait 30
      disable_retry_limit
      num_threads 8
    </match>

  # Define the container logs we will be looking at as well as setup the proper filters for them
  kubernetes.conf: |
    # https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter/blob/master/README.md
    <match fluent.**>
      type null
    </match>

    ###
    # Setup Container log push
    ###
    <source>
      type tail
      path /var/log/containers/*.log
      pos_file /var/log/fluentd-containers.log.pos
      time_format %Y-%m-%d T%H:%M:%S
      tag_to_kubernetes_name_regexp \.(?<pod_name>[^\._]+)_(?<namespace>[^_]+)_(?<container_name>.+)-(?<docker_id>[a-z0-9]{64})\.log$</pod>)
      tag kubernetes.*
      format json
      read_from_head true
    </source>

    ###
    # Setup log endpoints for Diamanti System Logs
    ###
    <source>
      @id docker.log
      type tail
      time_format %Y-%m-%dT %H:%M:%S
      format /^time="(?<time>[^)]*)" level=(?<severity>[^ ]*) msg="(?<message>[^"]*)"( err="(?<error>[^"]*)")?( statusCode=($<status_code>\d+))?/
      #format none
      path /var/log/diamanti/core/docker.log
      pos_file /var/log/fluentd-docker.log.pos
      tag docker
    </source>

    <source>
      @id etcd.log
      type tail
      format none
      time_format %a %d %H:%M:%S
      path /var/log/diamanti/core/etcd.log
      pos_file /var/log/fluentd-etcd.log.pos
      tag etcd
    </source>

    <source>
      @id armada.log
      type tail
      format none
      time_format %a %d %H:%M:%S
      path /var/log/diamanti/core/armada.log
      pos_file /var/log/fluentd-armada.log.pos
      tag armada
    </source>

    <source>
      @id convoy.log
      type tail
      format none
      time_format %a %d %H:%M:%S
      path /var/log/diamanti/core/convoy.log
      pos_file /var/log/fluentd-convoy.log.pos
      tag convoy
    </source>

    <source>
      @id bosun.log
      type tail
      format none
      time_format %a %d %H:%M:%S
      path /var/log/diamanti/core/bosun.log
      pos_file /var/log/fluentd-bosun.log.pos
      tag bosun
    </source>

    <source>
      @id dock.log
      type tail
      format none
      time_format %a %d %H:%M:%S
      path /var/log/diamanti/core/dock.log
      pos_file /var/log/fluentd-dock.log.pos
      tag dock
    </source>

    <source>
      @id ldfs.log
      type tail
      format none
      time_format %a %d %H:%M:%S
      path /var/log/diamanti/embedded/ldfs.log
      pos_file /var/log/fluentd-ldfs.log.pos
      tag ldfs
    </source>

    <source>
      @id nq_ach.log
      type tail
      format none
      time_format %a %d %H:%M:%S
      path /var/log/diamanti/embedded/nq_ach.log
      pos_file /var/log/fluentd-nq_ach.log.pos
      tag nq_ach
    </source>

    <source>
      @id scd.log
      type tail
      format none
      time_format %a %d %H:%M:%S
      path /var/log/diamanti/embedded/scd.log
      pos_file /var/log/fluentd-scd.log.pos
      tag scd
    </source>

    <source>
      @id skipper.log
      type tail
      format none
      time_format %a %d %H:%M:%S
      path /var/log/diamanti/embedded/skipper.log
      pos_file /var/log/fluentd-skipper.log.pos
      tag skipper
    </source>

    ###
    # Setup log endpoints for Diamanti Kubernetes Logs
    ###
    <source>
      @id kubelet.log
      type tail
      format kubernetes
      time_format %a %d %H:%M:%S
      multiline_flush_interval 5s
      path /var/log/diamanti/kubernetes/kubelet.log
      pos_file /var/log/fluentd-kubelet.log.pos
      tag kubelet
    </source>

    <source>
      @id kube-proxy.log
      type tail
      format kubernetes
      time_format %a %d %H:%M:%S
      multiline_flush_interval 5s
      path /var/log/diamanti/kubernetes/proxy.log
      pos_file /var/log/fluentd-kube-proxy.log.pos
      tag kube-proxy
    </source>

    <source>
      @id kube-apiserver.log
      type tail
      format kubernetes
      time_format %a %d %H:%M:%S
      multiline_flush_interval 5s
      path /var/log/diamanti/kubernetes/apiserver.log
      pos_file /var/log/fluentd-kube-apiserver.log.pos
      tag kube-apiserver
    </source>

    <source>
      @id controller-manager.log
      type tail
      format kubernetes
      time_format %a %d %H:%M:%S
      multiline_flush_interval 5s
      path /var/log/diamanti/kubernetes/controller-manager.log
      pos_file /var/log/fluentd-kube-controller-manager.log.pos
      tag kube-controller-manager
    </source>

    <source>
      @id kube-scheduler.log
      type tail
      format kubernetes
      time_format %a %d %H:%M:%S
      multiline_flush_interval 5s
      path /var/log/diamanti/kubernetes/scheduler.log
      pos_file /var/log/fluentd-kube-scheduler.log.pos
      tag kube-scheduler
    </source>

    <filter kubernetes.**>
      type kubernetes_metadata
    </filter>

    # The following two only work if you keep the default ingress controller names
    # but it is a good example of how to parse specific logs we might want to process
    # <filter kubernetes.var.log.containers.nginx-ingress-controller-**>
    #   # https://github.com/kubernetes/ingress/tree/master/controllers/nginx#log-format
    #   type parser
    #   reserve_data true
    #   hash_value_field parsed
    #   key_name log
    #   format /^(?<remote_addr>[^ ]*) - \[(?<proxy_add_x_forwarded_for>[^\]]*)\] - (?<remote_user>[^ ]*) \[(?<time_local>[^\]]*)\] "(?<method>\S+)(?: +(?<path>[^\"]*) +\S*)?" (?<status>[^ ]*) (?<body_bytes_sent>[^ ]*)(?: "(?<http_referer>[^\"]*)" "(?<http_user_agent>[^\"]*)")?.*$/
    # </filter>
    #
    # <filter kubernetes.var.log.containers.nginx-ingress-controller-**>
    #   type record_transformer
    #   <record>
    #     message "#{parsed.method} #{parsed.path} #{parsed.status}"
    #   </record>
    # </filter>

    ###
    # Setup log endpoints for Diamanti Plugin Logs
    ###
    <source>
      @id cni-network.log
      type tail
      format kubernetes
      time_format %a %d %H:%M:%S
      path /var/log/diamanti/plugn/cni-network.log
      pos_file /var/log/fluentd-cni-network.log.pos
      tag cni-network
    </source>

    <source>
      @id volume.log
      type tail
      format kubernetes
      time_format %a %d %H:%M:%S
      path /var/log/diamanti/plugn/volume.log
      pos_file /var/log/fluentd-volume.log.pos
      tag volume
    </source>

  # # Add prometheus endpoint so an external instance can query for useful data to analyze
  # prometheus.conf: |
  #   # https://github.com/kazegusuri/fluent-plugin-prometheus#usage
  #
  #   <source>
  #     @type prometheus
  #   </source>
  #
  #   <source>
  #     @type prometheus_monitor
  #   </source>
  #
  #   <filter **>
  #     @type prometheus
  #     <metric>
  #       name fluentd_records_total
  #       type counter
  #       desc The total number of records read by fluentd.
  #     </metric>
  #   </filter>

# Setup the FluentD DaemonSet in order to push collection pods to all nodes in the cluster
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: logging
  labels:
    app: fluentd
spec:
  template:
    metadata:
      name: fluentd
      labels:
        app: fluentd
    spec:
      serviceAccountName: fluentd
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:stable-elasticsearch
        # Can use below as a workaround a bug with configmap change in kubre 1.10 for now
        # command:
        #    - fluentd
        #    - -c
        #    - /fluentd/etc/fluent.conf
        imagePullPolicy: IfNotPresent
        env:
        - name:  FLUENT_ELASTICSEARCH_HOST
          value: "elasticsearch"
        - name:  FLUENT_ELASTICSEARCH_PORT
          value: "9200"
        - name:  FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX
          value: "fluentd"
        - name: DEBUG
          value: "false"
        # Add the USER/PASS and set to none to work around a bug in image for now, see note above for command
        - name: FLUENT_ELASTICSEARCH_USER
          value: none
        - name: FLUENT_ELASTICSEARCH_PASSWORD
          value: none
        volumeMounts:
        - name: config
          mountPath: /fluentd/etc
        - name: host-var-log
          mountPath: /var/log
        - name: host-var-lib-docker-containers
          mountPath: /var/lib/docker/containers
        - name: host-diamanti-log
          mountPath: /var/log/diamanti
        securityContext:
          runAsUser: 0
      volumes:
      - name: config
        configMap:
          name: fluentd
      - name: host-var-log
        hostPath:
          path: /var/log
      - name: host-var-lib-docker-containers
        hostPath:
          path: /var/lib/docker/containers
      - name: host-diamanti-log
        hostPath:
          path: /data/log

# Setup the Kibana ConfigMap
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: kibana
  namespace: logging
  labels:
    app: kibana
    stack: logging
data:
  kibana.yml: |
    server:
      name: "Diamanti-EFK-Example"
      port: 127.0.0.1:5601
    elasticsearch.url: "http://elasticsearch:9200"

# Setup the Kibana Deployment
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: kibana
  namespace: logging
  labels:
    app: kibana
    stack: logging
spec:
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: kibana
        stack: logging
    spec:
      # TODO: healthcheck + resources
      containers:
      - name: kibana
        # TODO: need to find a good way to keep the version updated on this as elastic doesnt provide a stable or "latest" tag
        image: docker.elastic.co/kibana/kibana-oss:6.2.3
        imagePullPolicy: IfNotPresent
        env:
        - name: ELASTICSEARCH_PASSWORD
          value: none
        ports:
        - containerPort: 5601
        resources: {}
        volumeMounts:
        - name: config
          mountPath: /usr/share/kibana/kibana.yml
          subPath: kibana.yml
      restartPolicy: Always
      volumes:
      - name: config
        configMap:
          name: kibana

# Setup the Kibana Ingress and EndPoint
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kibana
  namespace: logging
  labels:
    app: kibana
    stack: logging
  annotations:
    ingress.kubernetes.io/auth-signin: https://$host/oauth2/start
    ingress.kubernetes.io/auth-url: https://$host/oauth2/auth
    kubernetes.io/tls-acme: "true"
spec:
  rules:
  - host: kibana.logging.svc.solutions.eng.diamanti.com
    http:
      paths:
      - path: /
        backend:
          serviceName: kibana
          servicePort: 5601
  tls:
  - secretName: kibana-tls
    hosts:
    - kibana.logging.svc.solutions.eng.diamanti.com

# Setup the Kibana OAuth2 Proxy Ingress and EndPoint
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kibana-auth
  namespace: logging
  labels:
    app: kibana-auth
    stack: logging
  annotations:
    kubernetes.io/tls-acme: "true"
spec:
  rules:
  - host: kibana.logging.svc.solutions.eng.diamanti.com
    http:
      paths:
      - path: /oauth2
        backend:
          serviceName: oauth2-proxy
          servicePort: 4180
  tls:
  - secretName: kibana-tls
    hosts:
    - kibana.logging.svc.solutions.eng.diamanti.com

# Setup the Kibana Service
---
apiVersion: v1
kind: Service
metadata:
  name: kibana
  namespace: logging
  labels:
    app: kibana
    stack: logging
spec:
  clusterIP: None
  type: ClusterIP
  ports:
  - name: "ui"
    port: 5601
    targetPort: 5601
  selector:
    app: kibana
    stack: logging

# Setup the OAuth Proxy ConfigMap
# TODO: need to talk about what we want to do about Authentication as securing Kibana and Access Control is not that simple
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: oauth2-proxy
  namespace: logging
  labels:
    app: oauth2-proxy
data:
  # Example for using GitHub as the auth location
  oauth2_proxy.cfg: |
    # for reference see https://github.com/bitly/oauth2_proxy/blob/master/contrib/oauth2_proxy.cfg.example
    http_address = "0.0.0.0:4180"
    upstreams = ["file:///dev/null"]
    provider = "github"
    email_domains = ["*"]
    github_org = "diamntisolutions"
    github_team = "kube-admin,hacker"
    client_id = "16cf63d2612dc04c938f"
    client_secret = "e1ef428865fa6f05c6256d20a7bccdd027392fc2"
    cookie_secret = "vJsbnX5uFU0KY5OYa6a6gA=="

# Setup the OAuth Proxy Deployment
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: oauth2-proxy
  namespace: logging
  labels:
    app: oauth2-proxy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: oauth2-proxy
  template:
    metadata:
      labels:
        app: oauth2-proxy
    spec:
      containers:
      - name: oauth2-proxy
        image: giantswarm/oauth2_proxy:master
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 4180
          protocol: TCP
        volumeMounts:
        - name: config
          mountPath: /etc/oauth2_proxy
      volumes:
      - name: config
        configMap:
          name: oauth2-proxy

# Setup the OAuth Proxy Service
---
apiVersion: v1
kind: Service
metadata:
  name: oauth2-proxy
  namespace: logging
  labels:
    app: oauth2-proxy
spec:
  ports:
  - name: http
    port: 4180
    protocol: TCP
    targetPort: 4180
  selector:
    app: oauth2-proxy
